{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!if [ ! -e \"malevis_train_val_224x224\" ]; then \\\n",
        "  gdown --id '1HbTPEKsi8CCzor_mpRfh1KBQE52FZVXS' --output \"malevis_train_val_224x224.zip\" ;\\\n",
        "  unzip -q \"malevis_train_val_224x224.zip\" ;\\\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk-AFpK_V0lO",
        "outputId": "9d78ce92-6be5-4f1a-ee49-c25e24ab8db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HbTPEKsi8CCzor_mpRfh1KBQE52FZVXS\n",
            "To: /content/malevis_train_val_224x224.zip\n",
            "100% 1.74G/1.74G [00:10<00:00, 168MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MroGsm72Drb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset2 = ImageFolder(root=\"/content/malevis_train_val_224x224/train\")\n",
        "val_dataset2 = ImageFolder(root=\"/content/malevis_train_val_224x224/val\")"
      ],
      "metadata": {
        "id": "UiL70UVyWy7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lkPrGAgW__D",
        "outputId": "999d7e2c-55ab-4496-c748-2dfd184a1521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 9100\n",
              "    Root location: /content/malevis_train_val_224x224/train"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyqUJntIXWWP",
        "outputId": "1cc65825-5d96-4f4b-c311-4d561025a292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 5126\n",
              "    Root location: /content/malevis_train_val_224x224/val"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset2.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOetVoe4XCol",
        "outputId": "48d1618e-a998-4444-a9e0-8ab07ca54972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adposhel',\n",
              " 'Agent',\n",
              " 'Allaple',\n",
              " 'Amonetize',\n",
              " 'Androm',\n",
              " 'Autorun',\n",
              " 'BrowseFox',\n",
              " 'Dinwod',\n",
              " 'Elex',\n",
              " 'Expiro',\n",
              " 'Fasong',\n",
              " 'HackKMS',\n",
              " 'Hlux',\n",
              " 'Injector',\n",
              " 'InstallCore',\n",
              " 'MultiPlug',\n",
              " 'Neoreklami',\n",
              " 'Neshta',\n",
              " 'Other',\n",
              " 'Regrun',\n",
              " 'Sality',\n",
              " 'Snarasite',\n",
              " 'Stantinko',\n",
              " 'VBA',\n",
              " 'VBKrypt',\n",
              " 'Vilsel']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset2.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FrN-oXxXZE0",
        "outputId": "a3652491-20ea-4bcd-cdd4-7122e5dccf6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adposhel',\n",
              " 'Agent',\n",
              " 'Allaple',\n",
              " 'Amonetize',\n",
              " 'Androm',\n",
              " 'Autorun',\n",
              " 'BrowseFox',\n",
              " 'Dinwod',\n",
              " 'Elex',\n",
              " 'Expiro',\n",
              " 'Fasong',\n",
              " 'HackKMS',\n",
              " 'Hlux',\n",
              " 'Injector',\n",
              " 'InstallCore',\n",
              " 'MultiPlug',\n",
              " 'Neoreklami',\n",
              " 'Neshta',\n",
              " 'Other',\n",
              " 'Regrun',\n",
              " 'Sality',\n",
              " 'Snarasite',\n",
              " 'Stantinko',\n",
              " 'VBA',\n",
              " 'VBKrypt',\n",
              " 'Vilsel']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "train_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.Resize((256, 256)),\n",
        "    # You may add some transforms below\n",
        "    # TODO: some data augmentation?\n",
        "    # becomes 0 to 1\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=(0, 180)),    \n",
        "    transforms.ToTensor(),\n",
        "    # for pytorch pre-trained model\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# We don't need augmentations in testing and validation.\n",
        "# All we need here is to resize the PIL image and transform it into Tensor.\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "AmsAZx4lXlBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset2 = ImageFolder(root=\"/content/malevis_train_val_224x224/train\", transform=train_tfm)\n",
        "val_dataset2 = ImageFolder(root=\"/content/malevis_train_val_224x224/val\", transform=test_tfm)"
      ],
      "metadata": {
        "id": "WvR46eI-XmOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset2, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset2, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "J15XMbZQXyfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXvFj-0S2fnr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "class MalNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.backbone = resnet50(pretrained=pretrained)        \n",
        "        self.backbone.fc = nn.Linear(2048, 26)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6Neuffc2h9m",
        "outputId": "79492947-4b22-4fca-ec7d-838bda750aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 256, 256]) torch.Size([16, 26])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(16, 3, 256, 256)\n",
        "\n",
        "model = MalNet()\n",
        "y = model(x)\n",
        "\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdgAAHmt2jnl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_pretrained = MalNet(pretrained=True)\n",
        "model_pretrained = model_pretrained.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer_pretrained = optim.Adam(model_pretrained.parameters(), lr=0.0001)\n",
        "optimizer_pretrained = optim.Adam(model_pretrained.parameters(), lr=0.0001, weight_decay=0.00001)\n",
        "schedular = OneCycleLR(optimizer_pretrained, max_lr=0.0005, epochs=50, steps_per_epoch=len(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMbAmBQP2lml"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "losses = []\n",
        "accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "model_pretrained.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "    \n",
        "    running_losses = []\n",
        "    running_accs = []\n",
        "    val_running_losses = []\n",
        "    val_running_accs = []\n",
        "    for batch_idx, (imgs, labels) in enumerate(train_loader):    \n",
        "        imgs = imgs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        optimizer_pretrained.zero_grad()\n",
        "        outputs = model_pretrained(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_pretrained.step()\n",
        "        schedular.step()\n",
        "\n",
        "        preds = torch.max(outputs, dim=1)[1]\n",
        "        corrects = torch.sum(preds == labels).item()\n",
        "        acc = corrects/imgs.size(0)\n",
        "\n",
        "        running_losses.append(loss.item())\n",
        "        running_accs.append(acc)\n",
        "\n",
        "    model_pretrained.eval()\n",
        "    for batch_idx, (imgs, labels) in enumerate(val_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        \n",
        "        outputs = model_pretrained(imgs)\n",
        "        loss = criterion(outputs, labels)        \n",
        "\n",
        "        preds = torch.max(outputs, dim=1)[1]\n",
        "        corrects = torch.sum(preds == labels).item()\n",
        "        acc = corrects/imgs.size(0)\n",
        "\n",
        "        val_running_losses.append(loss.item())\n",
        "        val_running_accs.append(acc)\n",
        "\n",
        "    print('epoch: {}  |  training loss: {}  |  training accuracy: {}'.format(\n",
        "          epoch+1, sum(running_losses)/len(running_losses), sum(running_accs)/len(running_accs) ) )\n",
        "    print('epoch: {}  |  valing loss: {}  |  valing accuracy: {}'.format(\n",
        "          epoch+1, sum(val_running_losses)/len(val_running_losses), sum(val_running_accs)/len(val_running_accs) ) )    \n",
        "    #print(f\"Epoch {epoch}: {sum(running_losses)/len(running_losses):.3f}, {sum(running_accs)/len(running_accs)}\")\n",
        "    #print(f\"Epoch {epoch}: {sum(val_running_losses)/len(val_running_losses):.3f}, {sum(val_running_accs)/len(val_running_accs)}\")\n",
        "    losses.append(sum(running_losses)/len(running_losses))\n",
        "    accs.append(sum(running_accs)/len(running_accs))\n",
        "    val_losses.append(sum(val_running_losses)/len(val_running_losses))\n",
        "    val_accs.append(sum(val_running_accs)/len(val_running_accs))\n",
        "    \n",
        "plt.plot(np.arange(len(losses)), losses, label='Loss')\n",
        "plt.plot(np.arange(len(accs)), accs, label='Acc')\n",
        "plt.plot(np.arange(len(val_losses)), val_losses, label='Val Loss')\n",
        "plt.plot(np.arange(len(val_accs)), val_accs, label='Val Acc')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "model_pretrained.eval()\n",
        "    \n",
        "with torch.no_grad():\n",
        "    for i, (imgs, target) in enumerate(val_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        target = target.to(DEVICE)\n",
        "        output = model_pretrained(imgs)\n",
        "        _, preds = torch.max(output, 1)      #preds是預測結果\n",
        "        loss = criterion(output, target)          \n",
        "        y_pred.extend(preds.view(-1).detach().cpu().numpy())       \n",
        "        y_true.extend(target.view(-1).detach().cpu().numpy())\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)"
      ],
      "metadata": {
        "id": "s1XD1yO9NVc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "class_names = val_dataset2.classes\n",
        "df_cm = pd.DataFrame(cf_matrix, class_names, class_names) \n",
        "plt.figure(figsize = (10,9))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"label (ground truth)\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "kxO59yNmPHZU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Malevis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}